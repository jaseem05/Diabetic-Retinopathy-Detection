{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset succesfully split into train and validation sets✅!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define source and destination paths\n",
    "dataset_path=r\"C:\\Users\\Administrator\\Desktop\\DRD\\archive (16)\\gaussian_filtered_images\\gaussian_filtered_images\"\n",
    "trian_path=r\"C:\\Users\\Administrator\\Desktop\\DRD\\train\"\n",
    "val_path=r\"C:\\Users\\Administrator\\Desktop\\DRD\\val\"\n",
    "\n",
    "categories=[\"No_DR\" ,\"Mild\" ,\"Moderate\" ,\"Severe\" ,\"Proliferate_DR\"]\n",
    "\n",
    "# Split ratio\n",
    "split_ratio=0.8    # 80% tarin, 20% test\n",
    "\n",
    "# create trian and val directories\n",
    "for category in categories:\n",
    "    os.makedirs(os.path.join(trian_path,category),exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_path,category),exist_ok=True)\n",
    "\n",
    "    # Get all images in category folder\n",
    "    images=os.listdir(os.path.join(dataset_path,category))\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Splitdata\n",
    "    train_size=int(len(images)* split_ratio)\n",
    "    train_images=images[:train_size]\n",
    "    val_images=images[train_size:]\n",
    "\n",
    "    # Move images to train and val folders\n",
    "    for img in train_images:\n",
    "        shutil.move(os.path.join(dataset_path,category,img),os.path.join(trian_path,category,img))\n",
    "\n",
    "    for img in val_images:\n",
    "        shutil.move(os.path.join(dataset_path,category,img),os.path.join(val_path,category,img))\n",
    "\n",
    "print(\"Dataset succesfully split into train and validation sets✅!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2929 images belonging to 5 classes.\n",
      "Found 733 images belonging to 5 classes.\n",
      "Class Indices: {'No_DR': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate_DR': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 940ms/step - accuracy: 0.4897 - loss: 1.8667 - val_accuracy: 0.6917 - val_loss: 0.8718\n",
      "Epoch 2/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 914ms/step - accuracy: 0.6751 - loss: 0.8978 - val_accuracy: 0.6930 - val_loss: 0.8188\n",
      "Epoch 3/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 913ms/step - accuracy: 0.7163 - loss: 0.8013 - val_accuracy: 0.6903 - val_loss: 0.8050\n",
      "Epoch 4/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 920ms/step - accuracy: 0.6965 - loss: 0.8372 - val_accuracy: 0.6985 - val_loss: 0.8027\n",
      "Epoch 5/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 911ms/step - accuracy: 0.6847 - loss: 0.8509 - val_accuracy: 0.7149 - val_loss: 0.8056\n",
      "Epoch 6/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 921ms/step - accuracy: 0.6945 - loss: 0.8259 - val_accuracy: 0.7067 - val_loss: 0.8223\n",
      "Epoch 7/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 912ms/step - accuracy: 0.7032 - loss: 0.8082 - val_accuracy: 0.7231 - val_loss: 0.7700\n",
      "Epoch 8/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 912ms/step - accuracy: 0.7096 - loss: 0.7843 - val_accuracy: 0.7285 - val_loss: 0.8050\n",
      "Epoch 9/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 917ms/step - accuracy: 0.7150 - loss: 0.7719 - val_accuracy: 0.7135 - val_loss: 0.8069\n",
      "Epoch 10/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 915ms/step - accuracy: 0.7264 - loss: 0.7615 - val_accuracy: 0.7408 - val_loss: 0.7762\n",
      "Epoch 11/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 912ms/step - accuracy: 0.7240 - loss: 0.7490 - val_accuracy: 0.7422 - val_loss: 0.7374\n",
      "Epoch 12/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 914ms/step - accuracy: 0.7360 - loss: 0.7472 - val_accuracy: 0.7285 - val_loss: 0.7424\n",
      "Epoch 13/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 913ms/step - accuracy: 0.7423 - loss: 0.7039 - val_accuracy: 0.7367 - val_loss: 0.7319\n",
      "Epoch 14/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 921ms/step - accuracy: 0.7423 - loss: 0.7487 - val_accuracy: 0.7394 - val_loss: 0.7308\n",
      "Epoch 15/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 920ms/step - accuracy: 0.7465 - loss: 0.7096 - val_accuracy: 0.7422 - val_loss: 0.7805\n",
      "Epoch 16/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 912ms/step - accuracy: 0.7420 - loss: 0.7302 - val_accuracy: 0.7408 - val_loss: 0.7263\n",
      "Epoch 17/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 909ms/step - accuracy: 0.7304 - loss: 0.7239 - val_accuracy: 0.7367 - val_loss: 0.7656\n",
      "Epoch 18/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 910ms/step - accuracy: 0.7372 - loss: 0.7147 - val_accuracy: 0.7422 - val_loss: 0.6891\n",
      "Epoch 19/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 917ms/step - accuracy: 0.7538 - loss: 0.6729 - val_accuracy: 0.7367 - val_loss: 0.7180\n",
      "Epoch 20/20\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 917ms/step - accuracy: 0.7648 - loss: 0.6581 - val_accuracy: 0.7517 - val_loss: 0.7414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "train_dir = r\"C:\\Users\\Administrator\\Desktop\\DRD\\train\"\n",
    "val_dir = r\"C:\\Users\\Administrator\\Desktop\\DRD\\val\"\n",
    "\n",
    "# Define the correct class order\n",
    "class_mapping = {\"No_DR\": 0, \"Mild\": 1, \"Moderate\": 2, \"Severe\": 3, \"Proliferate_DR\": 4}\n",
    "\n",
    "# Load dataset with the correct class order\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, \n",
    "    target_size=(224, 224), \n",
    "    batch_size=32, \n",
    "    class_mode=\"categorical\",  # Multi-class classification\n",
    "    classes=class_mapping  # ✅ Manually specify class indices\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir, \n",
    "    target_size=(224, 224), \n",
    "    batch_size=32, \n",
    "    class_mode=\"categorical\",\n",
    "    classes=class_mapping  # ✅ Apply same mapping to validation data\n",
    ")\n",
    "\n",
    "# Print class indices to verify\n",
    "print(\"Class Indices:\", train_generator.class_indices)\n",
    "\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(5, activation=\"softmax\")  # ✅ 5 classes, softmax activation for multi-class\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])  # ✅ Correct loss function\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=20)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(r\"C:\\Users\\Administrator\\Desktop\\DRD\\dr.h5\")\n",
    "print(\"✅ Model training complete and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'No_DR': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate_DR': 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Prediction: No_DR\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the trained model\n",
    "model_path = r\"C:\\Users\\Administrator\\Desktop\\DRD\\dr.h5\"\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Class index mapping as per your custom label order\n",
    "class_labels = {\n",
    "    0: \"No_DR\",\n",
    "    1: \"Mild\",\n",
    "    2: \"Moderate\",\n",
    "    3: \"Severe\",\n",
    "    4: \"Proliferate_DR\"\n",
    "}\n",
    "\n",
    "# Load and preprocess image\n",
    "img_path = r\"C:\\Users\\Administrator\\Desktop\\DRD\\val\\Mild\\4e0656629d02.png\"  # Change to your image path\n",
    "img = image.load_img(img_path, target_size=(224, 224))  # Use target_size that matches model input\n",
    "img_array = image.img_to_array(img) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(img_array)\n",
    "predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "# Output result\n",
    "print(f\"Prediction: {class_labels[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
